{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMNiBTjFiMzDuC9PCGhyox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TapasKumarDutta1/multilingial/blob/master/multilingual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q85YUczOD32b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "from tensorflow.keras.layers import *\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "\n",
        "def regular_encode(texts, tokenizer, maxlen=512):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        return_attention_masks=False,\n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen,\n",
        "    )\n",
        "\n",
        "    return np.array(enc_di[\"input_ids\"])\n",
        "\n",
        "\n",
        "def build_model(transformer, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation=\"sigmoid\")(cls_token)\n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr=1e-5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print(\"Running on TPU \", tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
        "\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
        "batch = 2048\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "MAX_LEN = 192\n",
        "MODEL = \"jplu/tf-xlm-roberta-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "include_test = False\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    trn = pd.read_csv(\n",
        "        \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\",\n",
        "        usecols=[\"toxic\", \"comment_text\"],\n",
        "    )\n",
        "    val = pd.read_csv(\n",
        "        \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\",\n",
        "        usecols=[\"toxic\", \"comment_text\", \"lang\"],\n",
        "    )\n",
        "    tst = pd.read_csv(\n",
        "        \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\",\n",
        "        usecols=[\"lang\", \"content\"],\n",
        "    )\n",
        "    trn1 = pd.read_csv(\n",
        "        \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\",\n",
        "        usecols=[\"toxic\", \"comment_text\"],\n",
        "    )\n",
        "    trn[\"toxic\"] = trn[\"toxic\"].round().astype(int)\n",
        "    train = pd.concat(\n",
        "        [trn1, trn.query(\"toxic==1\"), trn.query(\"toxic==0\").sample(100000)], 0\n",
        "    )\n",
        "    train = train.reset_index(drop=True)\n",
        "\n",
        "    if include_test:\n",
        "        sub = pd.read_csv(\"../input/multilingual1/submission.csv\")\n",
        "        tst[\"toxic\"] = sub[\"toxic\"]\n",
        "        train = pd.concat([train, tst], 0).reset_index(drop=True)\n",
        "    return train, tst, val\n",
        "\n",
        "\n",
        "train, test, valid = load_data()\n",
        "\n",
        "LANGS = {\n",
        "    \"en\": \"english\",\n",
        "    \"it\": \"italian\",\n",
        "    \"fr\": \"french\",\n",
        "    \"es\": \"spanish\",\n",
        "    \"tr\": \"turkish\",\n",
        "    \"ru\": \"russian\",\n",
        "    \"pt\": \"portuguese\",\n",
        "}\n",
        "\n",
        "\n",
        "def get_sentences(text, lang=\"en\"):\n",
        "    return sent_tokenize(text, LANGS.get(lang, \"english\"))\n",
        "\n",
        "\n",
        "def exclude_duplicate_sentences(text, lang=\"en\"):\n",
        "    sentences = []\n",
        "    for sentence in get_sentences(text, lang):\n",
        "        sentence = sentence.strip()\n",
        "        if sentence not in sentences:\n",
        "            sentences.append(sentence)\n",
        "    return \" \".join(sentences)\n",
        "\n",
        "\n",
        "def clean_text(text, lang=\"en\"):\n",
        "    text = str(text)\n",
        "    text = re.sub(r'[0-9\"]', \"\", text)\n",
        "    text = re.sub(r\"#[\\S]+\\b\", \"\", text)\n",
        "    text = re.sub(r\"@[\\S]+\\b\", \"\", text)\n",
        "    text = re.sub(r\"https?\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = exclude_duplicate_sentences(text, lang)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "train[\"comment_text\"] = train.apply(lambda x: clean_text(x[\"comment_text\"]), axis=1)\n",
        "valid[\"comment_text\"] = valid.apply(\n",
        "    lambda x: clean_text(x[\"comment_text\"], x[\"lang\"]), axis=1\n",
        ")\n",
        "test[\"comment_text\"] = test.apply(lambda x: clean_text(x[\"content\"], x[\"lang\"]), axis=1)\n",
        "\n",
        "\n",
        "x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=192)\n",
        "x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=192)\n",
        "x_test = regular_encode(test.content.values, tokenizer, maxlen=192)\n",
        "\n",
        "y_train = train.toxic.values\n",
        "y_valid = valid.toxic.values\n",
        "\n",
        "\n",
        "train_set = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "validation_set = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        ")\n",
        "tst_set = tf.data.Dataset.from_tensor_slices((x_test)).batch(batch)\n",
        "\n",
        "trn_ln = train.shape[0]\n",
        "val_len = valid.shape[0]\n",
        "\n",
        "\n",
        "def build_model(transformer, max_len=512):\n",
        "    input_word_ids = Input(shape=(192,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    avg = AveragePooling1D(pool_size=192)(sequence_output)\n",
        "    tot = MaxPooling1D(pool_size=192)(sequence_output)\n",
        "    conc = Concatenate()([tot, avg])\n",
        "    out = Dense(100, activation=\"relu\")(conc)\n",
        "    drp = Dropout(0.3)(out)\n",
        "    conc = Flatten()(drp)\n",
        "    out_1 = Dense(1, activation=\"sigmoid\")(conc)\n",
        "\n",
        "    model = Model(inputs=input_word_ids, outputs=out_1)\n",
        "    model.summary()\n",
        "    model.compile(Adam(lr=1e-5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "with strategy.scope():\n",
        "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "    mod = build_model(transformer_layer, 192)\n",
        "mod.summary()\n",
        "\n",
        "mod.fit(\n",
        "    train_set,\n",
        "    steps_per_epoch=trn_ln // BATCH_SIZE,\n",
        "    validation_data=validation_set,\n",
        "    epochs=3,\n",
        ")\n",
        "\n",
        "mod.fit(validation_set, steps_per_epoch=val_len // BATCH_SIZE, epochs=2)\n",
        "\n",
        "sub = pd.read_csv(\n",
        "    \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\"\n",
        ")\n",
        "pre = mod.predict(tst_set, verbose=1)\n",
        "sub[\"toxic\"] = pre\n",
        "sub.to_csv(\"submission.csv\", index=False)\n"
      ]
    }
  ]
}