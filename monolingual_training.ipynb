{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TapasKumarDutta1/multilingial/blob/master/monolingual_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "id": "FmrNEi9-Hd7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "4xDGDxNCHfIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "metadata": {
        "id": "xEUM7PpKHgYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
        "from statistics import mean\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\n",
        "from torch.optim import *\n",
        "from torch.nn.modules.loss import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "from torch.utils.data.sampler import RandomSampler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import *\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from datetime import date\n",
        "import albumentations as A\n",
        "from tempfile import gettempdir\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import scipy as sp\n",
        "import cv2\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import random\n",
        "import argparse\n",
        "import sys\n",
        "import yaml\n",
        "import time\n",
        "import torch_xla\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "from typing import Dict\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch import Tensor\n",
        "from sklearn.metrics import *\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "import math"
      ],
      "metadata": {
        "id": "CLOLq9MTHtpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regular_encode(texts, tokenizer, maxlen=192):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, return_token_type_ids=False, pad_to_max_length=True, max_length=maxlen\n",
        "    )\n",
        "\n",
        "    return np.array(enc_di[\"input_ids\"])\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, transformer, num_classes=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.transformer = transformer\n",
        "\n",
        "        self.nb_features = self.transformer.pooler.dense.out_features\n",
        "        self.pooler = nn.Sequential(\n",
        "            nn.Linear(self.nb_features, num_classes),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        hidden_states = self.transformer(tokens, attention_mask=(tokens > 0).long())[1]\n",
        "\n",
        "\n",
        "        ft = self.pooler(hidden_states)\n",
        "\n",
        "        return ft\n"
      ],
      "metadata": {
        "id": "WxUl6s7DHxSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-30T13:40:53.371988Z",
          "iopub.status.busy": "2021-01-30T13:40:53.370948Z",
          "iopub.status.idle": "2021-01-30T13:40:53.374352Z",
          "shell.execute_reply": "2021-01-30T13:40:53.373807Z"
        },
        "papermill": {
          "duration": 0.055101,
          "end_time": "2021-01-30T13:40:53.374475",
          "exception": false,
          "start_time": "2021-01-30T13:40:53.319374",
          "status": "completed"
        },
        "tags": [],
        "id": "H3YP0NdpHbQI"
      },
      "outputs": [],
      "source": [
        "class bce(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(bce, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        one = (1 - targets) * torch.log(1 - inputs)\n",
        "        zero = targets * torch.log(inputs)\n",
        "        loss = torch.mean((one + zero) * -1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class JigsawDataset:\n",
        "    def __init__(self, x, y, is_test):\n",
        "        super().__init__()\n",
        "        self.y = y\n",
        "        self.is_test = is_test\n",
        "        self.sentences = x\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sentences.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        len = self.__len__()\n",
        "        if idx > len:\n",
        "            idx = idx % len\n",
        "        if self.is_test == 0:\n",
        "            return torch.tensor(self.sentences[idx]), torch.tensor(self.y[idx]).float()\n",
        "        else:\n",
        "            return torch.tensor(self.sentences[idx])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-30T13:40:53.452106Z",
          "iopub.status.busy": "2021-01-30T13:40:53.451407Z",
          "iopub.status.idle": "2021-01-30T13:40:54.967329Z",
          "shell.execute_reply": "2021-01-30T13:40:54.966745Z"
        },
        "papermill": {
          "duration": 1.556245,
          "end_time": "2021-01-30T13:40:54.967461",
          "exception": false,
          "start_time": "2021-01-30T13:40:53.411216",
          "status": "completed"
        },
        "tags": [],
        "id": "fltA6MlCHbQJ"
      },
      "outputs": [],
      "source": [
        "def train_all(train_loader, model, device, optimizer):\n",
        "    model.train()\n",
        "    model.train()\n",
        "    lss = bce()\n",
        "    loss1 = []\n",
        "    for step, (x, y_batch) in enumerate(train_loader):\n",
        "        y_batch = y_batch.to(device)\n",
        "        y_pred = model(x)\n",
        "\n",
        "        loss = lss(y_pred.view(-1).float(), y_batch.float())\n",
        "        loss.backward()\n",
        "        loss1.append(loss.item())\n",
        "        xm.optimizer_step(optimizer)\n",
        "\n",
        "        model.zero_grad()\n",
        "    return mean(loss1)\n",
        "\n",
        "\n",
        "def valid_all(train_loader, model, device):\n",
        "    lss = bce()\n",
        "    loss1 = []\n",
        "    for step, (x, y_batch) in enumerate(train_loader):\n",
        "        y_batch = y_batch.to(device)\n",
        "        y_pred = model(x)\n",
        "\n",
        "        loss = lss(y_pred.view(-1).float(), y_batch.float())\n",
        "        loss1.append(loss.item())\n",
        "\n",
        "    return mean(loss1)\n",
        "\n",
        "\n",
        "def predict_all(train_loader, model, device):\n",
        "    predict = []\n",
        "    for step, (x) in tqdm(enumerate(train_loader)):\n",
        "        y_pred = model(x.to(device))\n",
        "        predict.append(y_pred)\n",
        "\n",
        "    return predict\n",
        "\n",
        "\n",
        "def load_data(lang):\n",
        "    trn = pd.read_csv(\n",
        "        \"../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-\"\n",
        "        + lang\n",
        "        + \"-cleaned.csv\",\n",
        "        usecols=[\"toxic\", \"comment_text\"],\n",
        "    )\n",
        "    trn[\"lang\"] = lang\n",
        "    tst = pd.read_csv(\n",
        "        \"../input/jigsaw-multilingual-toxic-comment-classification/test.csv\",\n",
        "        usecols=[\"lang\", \"content\"],\n",
        "    )\n",
        "    sub = pd.read_csv(\"../input/multilingual1/submission.csv\")\n",
        "    val = pd.read_csv(\n",
        "        \"../input/jigsaw-multilingual-toxic-comment-classification/validation.csv\",\n",
        "        usecols=[\"lang\", \"comment_text\", \"toxic\"],\n",
        "    )\n",
        "    tst.columns = [\"comment_text\", \"lang\"]\n",
        "    tst[\"toxic\"] = sub[\"toxic\"]\n",
        "    df = pd.concat([trn, tst, val], 0)\n",
        "    return df.loc[df[\"lang\"] == lang].reset_index(drop=True).drop([\"lang\"], 1)\n",
        "\n",
        "\n",
        "def get_lang(val, tst, lang):\n",
        "    df = pd.concat([val, tst], 0)\n",
        "    return df.loc[df[\"lang\"] == lang].reset_index(drop=True).drop([\"id\", \"lang\"], 1)\n",
        "\n",
        "\n",
        "def main():\n",
        "    l1 = \"fr\"\n",
        "    lang = \"french\"\n",
        "    link_dk = {\n",
        "        \"fr\": \"camembert-base\",\n",
        "        \"pt\": \"neuralmind/bert-base-portuguese-cased\",\n",
        "        \"ru\": \"DeepPavlov/rubert-base-cased\",\n",
        "        \"tr\": \"dbmdz/bert-base-turkish-cased\",\n",
        "        \"es\": \"dccuchile/bert-base-spanish-wwm-cased\",\n",
        "        \"it\": \"dbmdz/bert-base-italian-xxl-cased\",\n",
        "    }\n",
        "    epochs = 1\n",
        "    batch_size = 16\n",
        "    learning_rate = 1e-5\n",
        "    seed = 42\n",
        "\n",
        "    # Setting seed\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    df = load_data(l1)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(link_dk[l1])\n",
        "    x_train = regular_encode(list(df.comment_text.values), tokenizer, maxlen=192)\n",
        "    y_train = df.toxic.values\n",
        "\n",
        "    idx = df.loc[(df[\"toxic\"] > 0) & (df[\"toxic\"] < 1)].index\n",
        "    test = x_train[idx]\n",
        "\n",
        "    def run():\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        device = xm.xla_device()\n",
        "        model = AutoModel.from_pretrained(link_dk[l1])\n",
        "        model = Transformer(model).to(device)\n",
        "\n",
        "        # Training\n",
        "        train_dataset = JigsawDataset(trn_x, trn_y, 0)\n",
        "\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            train_dataset,\n",
        "            num_replicas=xm.xrt_world_size(),\n",
        "            rank=xm.get_ordinal(),\n",
        "            shuffle=False,\n",
        "        )\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=train_sampler,\n",
        "            drop_last=False,\n",
        "            num_workers=2,\n",
        "        )\n",
        "\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate * xm.xrt_world_size(),\n",
        "            weight_decay=1e-3,\n",
        "        )\n",
        "\n",
        "        xm.master_print(\"Training is Starting ...... \")\n",
        "        total_loss = []\n",
        "        valid_loss = []\n",
        "        predictions = []\n",
        "        for i in tqdm(range(3)):\n",
        "            para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "            total_loss.append(\n",
        "                train_all(\n",
        "                    para_loader.per_device_loader(device), model, device, optimizer\n",
        "                )\n",
        "            )\n",
        "\n",
        "        state = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
        "        xm.save(state, lang + str(number))\n",
        "\n",
        "    def _mp_fn(rank, flags):\n",
        "        torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
        "        run()\n",
        "\n",
        "    kf = KFold(n_splits=5, random_state=42, shuffle=False)\n",
        "    number = 0\n",
        "    for train_index, test_index in kf.split(range(df.shape[0])):\n",
        "        trn_x = x_train[train_index]\n",
        "        trn_y = y_train[train_index]\n",
        "        number += 1\n",
        "        FLAGS = {}\n",
        "        xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method=\"fork\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "qEcUzilPIF6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-30T16:36:38.598268Z",
          "iopub.status.busy": "2021-01-30T16:36:38.593029Z",
          "iopub.status.idle": "2021-01-30T16:42:59.335449Z",
          "shell.execute_reply": "2021-01-30T16:42:59.334814Z"
        },
        "papermill": {
          "duration": 380.885743,
          "end_time": "2021-01-30T16:42:59.335582",
          "exception": false,
          "start_time": "2021-01-30T16:36:38.449839",
          "status": "completed"
        },
        "tags": [],
        "id": "wgO6h9sPHbQJ",
        "outputId": "0c2f9778-2c92-4eb2-978c-88c72a235cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10920, 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "683it [01:15,  9.03it/s]                           \n",
            "683it [00:59, 11.55it/s]\n",
            "683it [00:59, 11.53it/s]\n",
            "683it [00:59, 11.51it/s]\n",
            "683it [00:59, 11.53it/s]\n"
          ]
        }
      ],
      "source": [
        "def predict_all(train_loader, model, device, df, batch_size):\n",
        "    predict = []\n",
        "    for step, (x) in tqdm(enumerate(train_loader), total=df.shape[0] / (batch_size)):\n",
        "        y_pred = model(x.to(device))\n",
        "        predict.append(y_pred.cpu().detach().numpy())\n",
        "\n",
        "    return predict\n",
        "\n",
        "\n",
        "def load_data(lang):\n",
        "    tst = pd.read_csv(\n",
        "        \"../input/jigsaw-multilingual-toxic-comment-classification/test.csv\",\n",
        "        usecols=[\"lang\", \"content\"],\n",
        "    )\n",
        "    tst = tst.loc[tst[\"lang\"] == lang].reset_index(drop=True).drop([\"lang\"], 1)\n",
        "    print(tst.shape)\n",
        "    return tst\n",
        "\n",
        "\n",
        "def get_lang(val, tst, lang):\n",
        "    df = pd.concat([val, tst], 0)\n",
        "    return df.loc[df[\"lang\"] == lang].reset_index(drop=True).drop([\"id\", \"lang\"], 1)\n",
        "\n",
        "\n",
        "def main():\n",
        "    epochs = 1\n",
        "    seed = 42\n",
        "    batch_size = 16\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    l1 = \"fr\"\n",
        "    lang = \"french\"\n",
        "    link_dk = {\n",
        "        \"fr\": \"camembert-base\",\n",
        "        \"pt\": \"neuralmind/bert-base-portuguese-cased\",\n",
        "        \"ru\": \"DeepPavlov/rubert-base-cased\",\n",
        "        \"tr\": \"dbmdz/bert-base-turkish-cased\",\n",
        "        \"es\": \"dccuchile/bert-base-spanish-wwm-cased\",\n",
        "        \"it\": \"dbmdz/bert-base-italian-xxl-cased\",\n",
        "    }\n",
        "    df = load_data(l1)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(link_dk[l1])\n",
        "    x_train = regular_encode(list(df.content.values), tokenizer, maxlen=192)\n",
        "\n",
        "    def run():\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        device = xm.xla_device()\n",
        "        model = AutoModel.from_pretrained(link_dk[l1])\n",
        "        model = Transformer(model).to(device)\n",
        "        model.load_state_dict(torch.load(lang + str(number))[\"state_dict\"])\n",
        "\n",
        "        train_dataset = JigsawDataset(x_train, None, 1)\n",
        "\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            train_dataset,\n",
        "            num_replicas=xm.xrt_world_size(),\n",
        "            rank=xm.get_ordinal(),\n",
        "            shuffle=False,\n",
        "        )\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            sampler=train_sampler,\n",
        "            drop_last=False,\n",
        "            num_workers=2,\n",
        "        )\n",
        "\n",
        "        predictions = []\n",
        "        para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "        predictions.append(\n",
        "            predict_all(\n",
        "                para_loader.per_device_loader(device), model, device, df, batch_size\n",
        "            )\n",
        "        )\n",
        "\n",
        "        np.save(lang + \"_predictions_\" + str(number) + \".npy\", predictions)\n",
        "\n",
        "    for number in range(1, 6):\n",
        "        run()\n",
        "\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 11050.379389,
      "end_time": "2021-01-30T16:43:00.237754",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-01-30T13:38:49.858365",
      "version": "2.1.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}